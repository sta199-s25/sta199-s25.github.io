---
title: "Lab 7"
date: "2025-3-31"
format: revealjs
auto-stretch: false
---

# Overview

```{r}
#| include: false
library(tidyverse)
library(tidymodels)
library(forested)
```

## Lab 7 Overview

-   Part 1: all things logistic regression

-   Part 2: a quick data science assessment survey

# Data Science Reasoning Assessment

## Data Science Reasoning Assessment {.medium}

::: incremental
-   The goal of this data science assessment, to accurately measure introductory data science students’ reasoning skills.

-   You will be graded based on completion + honest effort. This assessment is not optional. This is meant to be an individual, closed notes assessment.

-   At the end, you will be asked if your anonymized responses can be used to help better improve this assessment.

    -   Your responses matter!
    -   The goal of this assessment is to be a national data science reasoning assessment for introductory courses
    -   This starts with improving questions based on your responses
:::

# Logistic regression overview

## `forested` data

::: incremental
-   `r nrow(forested)` rows and `r ncol(forested)` columns;

-   Each observation (row) is a plot of land;

-   Variables include geographical and meteorological information about each plot, as well as a binary indicator `forested` ("Yes" or "No");

-   Given information about a plot that is easy (and cheap) to collect remotely, can we use a model to predict if a plot is forested without actually visiting it (which could be difficult and costly)?
:::

## Goal

::: incremental
-   Use the data we've already seen to predict if a yet-to-be-observed plot of land is forested;

-   We want a model that does well on data it has never seen before;

-   "Out-of-sample" predictions on new data are more useful than "in-sample" predictions on old data;
:::

## Training versus testing data

To mimic this "out-of-sample" idea, we randomly split the data into two parts:

-   **training data**: this is what the model gets to see when we fit it;
-   **test data**: withheld. We assess how well the trained model can predict on this data it hasn't seen before.

![](images/18/test-train-split-1.svg){fig-align="center"}

## Randomly split data into training and test sets

By default it's a 75%/25% training/test split.

```{r}
set.seed(8675309)

forested_split <- initial_split(forested)

forested_train <- training(forested_split)
forested_test <- testing(forested_split)
```

The split is random, but we want the results to be reproducible, so we "freeze the random numbers in time" by setting a seed. If we don't tell you exactly what seed to use on an assignment, you can pick any positive integer you want.

## Explore: forested or not {.medium}

```{r}
ggplot(forested_train, aes(x = lon, y = lat, color = forested)) +
  geom_point(alpha = 0.7) +
  scale_color_manual(values = c("Yes" = "forestgreen", "No" = "gold2")) +
  theme_minimal()
```

## Explore: annual precipitation {.medium}

```{r}
ggplot(forested_train, aes(x = lon, y = lat, color = precip_annual)) +
  geom_point(alpha = 0.7) +
  labs(color = "annual\nprecipitation\n(mm × 100)") +
  theme_minimal()
```

## FYI: the response variable *must* be a factor

`forested` already comes as a factor, so we're lucky:

```{r}
class(forested$forested)
levels(forested$forested)
```

But if it didn't, things would not work:

```{r}
#| error: true
logistic_reg() |>
  fit(as.numeric(forested) ~ precip_annual, data = forested_train)
```

## FYI: the base level is treated as "failure" (0) {.medium}

The base level here is "Yes", so "No" is treated as "success" (1):

```{r}
levels(forested$forested)
```

As a result, this code:

```{r}
#| eval: false
logistic_reg() |>
  fit(forested ~ precip_annual, data = forested_train)
```

Corresponds to this model:

$$
\text{Prob}(
\texttt{forested = "No"}
)
=
\frac{e^{\beta_0+\beta_1 x}}{1 + e^{\beta_0+\beta_1 x}}.
$$

This is not a problem, but it means that in order to interpret the output correctly, you need to understand how your factors are leveled.

## Fitting a logistic regression model

Similar syntax to linear regression:

```{r}
forested_precip_fit <- logistic_reg() |>
  fit(forested ~ precip_annual, data = forested_train)

tidy(forested_precip_fit)
```

$$
\log\left(\frac{\hat{p}}{1-\hat{p}}\right)
=
1.57
- 
0.0019
\times 
precip.
$$

## Interpreting the intercept

$$
\begin{aligned}
\log\left(\frac{\hat{p}}{1-\hat{p}}\right)
&=
1.57
- 
0.0019
\times 
precip\\
\frac{\hat{p}}{1-\hat{p}}
&=
e^{1.57
- 
0.0019
\times 
precip}
.
\end{aligned}
$$

So when $precip = 0$, the model predicts that the odds of `forested = "No"` are $e^{1.57}\approx 4.8$, on average.

## Interpreting the slope {.medium .scrollable}

At $precip$:

$$
\frac{\hat{p}}{1-\hat{p}}
=
{\color{blue}{e^{1.57
- 
0.0019
\times 
precip}}}
$$

At $precip + 1$:

$$
\begin{aligned}
\frac{\hat{p}}{1-\hat{p}}
&=
e^{1.57
- 
0.0019
\times 
(precip + 1)}
\\
&=
e^{1.57
- 
0.0019
\times 
precip - 0.0019}
\\
&=
{\color{blue}{e^{1.57
- 
0.0019
\times 
precip}}}
\times 
\color{red}{e^{-0.0019}}
\end{aligned}
$$

If $precip$ increases by one unit, the model predicts a *decrease* in the odds that `forested = "No"` by a multiplicative factor of $e^{-0.0019}\approx 0.99$, on average.

## Generate predictions for the test data {.small}

*Augment* the test data frame with three new columns on the left that include model predictions (classifications and probabilities) for each row:

```{r}
forested_precip_aug <- augment(forested_precip_fit, forested_test)
forested_precip_aug
```

## How did the model perform? {.medium}

These are the four possibilities:

![](images/labs/forested_errs.png){fig-align="center"}

-   Our test data have the truth in the `forested` column;
-   We can compare the predictions in `.pred_class` to the true values and see how we did.

## Getting the error rates {.medium}

```{r}
forested_precip_aug |>
  count(forested, .pred_class) |>
  group_by(forested) |>
  mutate(
    p = n / sum(n),
    decision = case_when(
      forested == "Yes" & .pred_class == "Yes" ~ "sensitivity",
      forested == "Yes" & .pred_class == "No" ~ "false negative",
      forested == "No" & .pred_class == "Yes" ~ "false positive",
      forested == "No" & .pred_class == "No" ~ "specificity",
    )
  )
```

## Change threshold to 0.00

New threshold \> New classifications \> New error rates

```{r}
#| code-line-numbers: 2-4
forested_precip_aug |>
  mutate(
    .pred_class = if_else(.pred_Yes <= 0.0, "No", "Yes")
  ) |>
  count(forested, .pred_class) |>
  group_by(forested) |>
  mutate(
    p = n / sum(n),
  )
```

## Change threshold to 0.25

New threshold \> New classifications \> New error rates

```{r}
#| code-line-numbers: 2-4
forested_precip_aug |>
  mutate(
    .pred_class = if_else(.pred_Yes <= 0.25, "No", "Yes")
  ) |>
  count(forested, .pred_class) |>
  group_by(forested) |>
  mutate(
    p = n / sum(n),
  )
```

## Change threshold to 0.50

New threshold \> New classifications \> New error rates

```{r}
#| code-line-numbers: 2-4
forested_precip_aug |>
  mutate(
    .pred_class = if_else(.pred_Yes <= 0.50, "No", "Yes")
  ) |>
  count(forested, .pred_class) |>
  group_by(forested) |>
  mutate(
    p = n / sum(n),
  )
```

## Change threshold to 0.75

New threshold \> New classifications \> New error rates

```{r}
#| code-line-numbers: 2-4
forested_precip_aug |>
  mutate(
    .pred_class = if_else(.pred_Yes <= 0.75, "No", "Yes")
  ) |>
  count(forested, .pred_class) |>
  group_by(forested) |>
  mutate(
    p = n / sum(n),
  )
```

## Change threshold to 1.00

New threshold \> New classifications \> New error rates

```{r}
#| code-line-numbers: 2-4
forested_precip_aug |>
  mutate(
    .pred_class = if_else(.pred_Yes <= 1.00, "No", "Yes")
  ) |>
  count(forested, .pred_class) |>
  group_by(forested) |>
  mutate(
    p = n / sum(n),
  )
```

## Picture how errors change with threshold

```{r}
#| echo: false
tibble(
  threshold = c(0, .25, .5, .75, 1),
  sensitivity = c(1, 1, 0.702, 0.513, 0),
  specificity = c(0, 0.223, 0.826, 0.909, 1)
) |>
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = "red")) + 
  geom_point() + 
  geom_abline(lty = 3) + 
  coord_equal() + 
  theme_minimal() + 
  theme(legend.position = "none")
```

## Picture how errors change with threshold

```{r}
#| echo: false
tibble(
  threshold = c(0, .25, .5, .75, 1),
  sensitivity = c(1, 1, 0.702, 0.513, 0),
  specificity = c(0, 0.223, 0.826, 0.909, 1)
) |>
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = "red")) + 
  geom_point() + 
  geom_abline(lty = 3) + 
  annotate("text", 
           x = c(1 - 0.05, 1 - 0.223 - 0.15, 1 - 0.826, 1 - 0.909, 0.15), 
           y = c(1 - 0.05, 1 , 0.702 + .1, 0.513 + .1, 0), 
           label = c("p* = 0.0", "p* = 0.25", "p* = 0.5", "p* = 0.75", "p* = 1.0")) + 
  coord_equal() + 
  theme_minimal() + 
  theme(legend.position = "none")
```

## But that was tedious

Let do "all" of the thresholds and connect the dots:

```{r}
forested_precip_roc <- roc_curve(forested_precip_aug, 
                                 truth = forested, 
                                 .pred_Yes, 
                                 event_level = "first")
forested_precip_roc
```

## The ROC curve

```{r}
ggplot(forested_precip_roc, aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() + 
  theme_minimal()
```

## The ROC curve

-   ROC stands for receiver operating characteristic;

-   This visualizes the classification accuracy across a range of thresholds. The more "up and to the left" it is, the better.

-   We can quantify "up and to the left" with the area under the curve (AUC).

## The ROC curve

![](images/18/roc-curve-annotated.png){fig-align="center"}

## AUC = 1

This is the best we could possibly do:

```{r}
#| echo: false

tibble(
  specificity = c(1, 1, 0), 
  sensitivity = c(0, 1, 1)
) |>
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() + 
  theme_minimal()
```

## AUC = 1 / 2

Don't waste time fitting a model. Just flip a coin:

```{r}
#| echo: false

tibble(
  specificity = c(1, 1/2, 0), 
  sensitivity = c(0, 1/2, 1)
) |>
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() + 
  theme_minimal()
```

## AUC = 0

This is the worst we could possibly do:

```{r}
#| echo: false

tibble(
  specificity = c(1, 0, 0), 
  sensitivity = c(0, 0, 1)
) |>
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() + 
  theme_minimal()
```

## AUC for the model we fit

```{r}
roc_auc(
  forested_precip_aug, 
  truth = forested, 
  .pred_Yes, 
  event_level = "first"
)
```

Not bad!

## The area under the ROC curve

-   This is a "quality score" for a logistic regression model;

-   When you compute it for a test data set that you set aside, the AUC is a measure of *out-of-sample* prediction accuracy;

-   AUC is a number between 0 and 1, where 0 is awful and 1 is great, similar to $R^2$ for linear regression.

-   The function `roc_auc` computes it for you, and it takes the same set of arguments as `roc_curve`.

## New commands introduced last week

-   `logistic_reg`
-   `augment`
-   `roc_curve`
-   `roc_auc`
-   `set.seed`
-   `initial_split`, `training`, `test`
-   `geom_path`

You will use them all on Lab 7, and they should go on your Midterm 2 cheat sheet.
