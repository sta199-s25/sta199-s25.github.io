---
title: "Lab 7"
date: "2025-3-31"
format: revealjs
auto-stretch: false
---

# Overview

```{r}
#| include: false
library(tidyverse)
library(tidymodels)
library(forested)
```

## Lab 7 Overview

-   Part 1

-   Part 2

# Logistic regression overview

## `forested` data

x rows and y columns. each observation is a spit of land. we have weather and geo info about it. is it forested or not. 

SOme areas are hard to reach, so given data about the location that can be collected remotely, we want to use a model to predict if that area is forested.


## Goal

Use the data we've already seen to predict if a yet-to-be-observed parcel of land is forested. 

We want a model that does well on data it has never seen before. 

To try this out, we randomly split the data into two parts

## Randomly split data into training and test sets

By default it's a 75/25 training/test split.

```{r}
set.seed(8675309)

forested_split <- initial_split(forested)

forested_train <- training(forested_split)
forested_test <- testing(forested_split)
```

The split is random, but we want the results to be reproducible, so we "freeze the random numbers in time" by setting a seed. If we don't tell you exactly what number to use, you can pick any integer you want. 

## Explore: forested or not {.medium}

```{r}
ggplot(forested_train, aes(x = lon, y = lat, color = forested)) +
  geom_point(alpha = 0.7) +
  scale_color_manual(values = c("Yes" = "forestgreen", "No" = "gold2")) +
  theme_minimal()
```

## Explore: annual precipitation {.medium}

```{r}
ggplot(forested_train, aes(x = lon, y = lat, color = precip_annual)) +
  geom_point(alpha = 0.7) +
  labs(color = "annual\nprecipitation\n(mm Ã— 100)") +
  theme_minimal()
```

## FYI: the response variable *must* be a factor

`forested` already comes as a factor, so we're lucky:

```{r}
class(forested$forested)
levels(forested$forested)
```

But if it didn't, things would not work:

```{r}
#| error: true
logistic_reg() |>
  fit(as.numeric(forested) ~ precip_annual, data = forested_train)
```

## FYI: the base level is treated as "failure" (0)

## Fitting a logistic regression model

Similar syntax to linear regression:

```{r}
forested_precip_fit <- logistic_reg() |>
  fit(forested ~ precip_annual, data = forested_train)

tidy(forested_precip_fit)
```

$$
\log\left(\frac{\hat{p}}{1-\hat{p}}\right)
=
1.57
- 
0.0019
\times 
precip.
$$

## Interpreting the intercept

$$
\log\left(\frac{\hat{p}}{1-\hat{p}}\right)
=
1.57
- 
0.0019
\times 
precip.
$$

## Interpreting the slope 

$$
\log\left(\frac{\hat{p}}{1-\hat{p}}\right)
=
1.57
- 
0.0019
\times 
precip.
$$

## Generate predictions for the test data {.small}

*Augment* the test data frame with three new columns on the left that include model predictions (classifications and probabilities) for each row:

```{r}
forested_precip_aug <- augment(forested_precip_fit, forested_test)
forested_precip_aug
```


## Getting the error rates {.medium}

```{r}
forested_precip_aug |>
  count(forested, .pred_class) |>
  group_by(forested) |>
  mutate(
    p = n / sum(n),
    decision = case_when(
      forested == "Yes" & .pred_class == "Yes" ~ "sensitivity",
      forested == "Yes" & .pred_class == "No" ~ "false negative",
      forested == "No" & .pred_class == "Yes" ~ "false positive",
      forested == "No" & .pred_class == "No" ~ "specificity",
    )
  )
```


## Change threshold to 0.00

New threshold > New classifications > New error rates

```{r}
#| code-line-numbers: 2-4
forested_precip_aug |>
  mutate(
    .pred_class = if_else(.pred_Yes <= 0.0, "No", "Yes")
  ) |>
  count(forested, .pred_class) |>
  group_by(forested) |>
  mutate(
    p = n / sum(n),
  )
```

## Change threshold to 0.25

New threshold > New classifications > New error rates

```{r}
#| code-line-numbers: 2-4
forested_precip_aug |>
  mutate(
    .pred_class = if_else(.pred_Yes <= 0.25, "No", "Yes")
  ) |>
  count(forested, .pred_class) |>
  group_by(forested) |>
  mutate(
    p = n / sum(n),
  )
```

## Change threshold to 0.50

New threshold > New classifications > New error rates

```{r}
#| code-line-numbers: 2-4
forested_precip_aug |>
  mutate(
    .pred_class = if_else(.pred_Yes <= 0.50, "No", "Yes")
  ) |>
  count(forested, .pred_class) |>
  group_by(forested) |>
  mutate(
    p = n / sum(n),
  )
```

## Change threshold to 0.75

New threshold > New classifications > New error rates

```{r}
#| code-line-numbers: 2-4
forested_precip_aug |>
  mutate(
    .pred_class = if_else(.pred_Yes <= 0.75, "No", "Yes")
  ) |>
  count(forested, .pred_class) |>
  group_by(forested) |>
  mutate(
    p = n / sum(n),
  )
```

## Change threshold to 1.00

New threshold > New classifications > New error rates

```{r}
#| code-line-numbers: 2-4
forested_precip_aug |>
  mutate(
    .pred_class = if_else(.pred_Yes <= 1.00, "No", "Yes")
  ) |>
  count(forested, .pred_class) |>
  group_by(forested) |>
  mutate(
    p = n / sum(n),
  )
```

## Picture how errors change with threshold

```{r}
#| echo: false
tibble(
  threshold = c(0, .25, .5, .75, 1),
  sensitivity = c(1, 1, 0.702, 0.513, 0),
  specificity = c(0, 0.223, 0.826, 0.909, 1)
) |>
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = "red")) + 
  geom_point() + 
  geom_abline(lty = 3) + 
  coord_equal() + 
  theme_minimal() + 
  theme(legend.position = "none")
```

## Picture how errors change with threshold

```{r}
#| echo: false
tibble(
  threshold = c(0, .25, .5, .75, 1),
  sensitivity = c(1, 1, 0.702, 0.513, 0),
  specificity = c(0, 0.223, 0.826, 0.909, 1)
) |>
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = "red")) + 
  geom_point() + 
  geom_abline(lty = 3) + 
  annotate("text", 
           x = c(1 - 0.05, 1 - 0.223 - 0.15, 1 - 0.826, 1 - 0.909, 0.15), 
           y = c(1 - 0.05, 1 , 0.702 + .1, 0.513 + .1, 0), 
           label = c("p* = 0.0", "p* = 0.25", "p* = 0.5", "p* = 0.75", "p* = 1.0")) + 
  coord_equal() + 
  theme_minimal() + 
  theme(legend.position = "none")
```


## But that was tedious

Let do "all" of the thresholds and connect the dots:

```{r}
forested_precip_roc <- roc_curve(forested_precip_aug, 
                                 truth = forested, 
                                 .pred_Yes, 
                                 event_level = "first")
forested_precip_roc
```

## The ROC curve

```{r}
ggplot(forested_precip_roc, aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() + 
  theme_minimal()
```

## The ROC curve

- ROC stands for receiver operating characteristic;

- This visualizes the classification accuracy across a range of thresholds. The more "up and to the left" it is, the better.

- We can quantify "up and to the left" with the area under the curve (AUC).

## The ROC curve

![](images/18/roc-curve-annotated.png){fig-align="center"}

## AUC = 1

This is the best we could possibly do: 

```{r}
#| echo: false

tibble(
  specificity = c(1, 1, 0), 
  sensitivity = c(0, 1, 1)
) |>
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() + 
  theme_minimal()
```

## AUC = 1 / 2

Don't waste time fitting a model. Just flip a coin:

```{r}
#| echo: false

tibble(
  specificity = c(1, 1/2, 0), 
  sensitivity = c(0, 1/2, 1)
) |>
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() + 
  theme_minimal()
```

## AUC = 0

This is the worst we could possibly do:

```{r}
#| echo: false

tibble(
  specificity = c(1, 0, 0), 
  sensitivity = c(0, 0, 1)
) |>
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() + 
  theme_minimal()
```

## AUC for the model we fit


```{r}
roc_auc(
  forested_precip_aug, 
  truth = forested, 
  .pred_Yes, 
  event_level = "first"
)
```

Not bad!

## The area under the ROC curve

- This is a "quality score" for a logistic regression model; 

- When you compute it for a test data set that you set aside, the AUC is a measure of *out-of-sample* prediction accuracy;

- AUC is a number between 0 and 1, where 0 is awful and 1 is great, similar to $R^2$ for linear regression.

- The function `roc_auc` computes it for you, and it takes the same set of arguments as `roc_curve`.

## New commands introduced last week

- `logistic_reg`
- `augment`
- `roc_curve`
- `roc_auc`
- `set.seed`
- `initial_split`, `training`, `test`
- `geom_path`

You will use them all on Lab 7, and they should go on your Midterm 2 cheat sheet.
